import torch
from pdp_generator import NODE_DEPOT, NODE_PICKUP, NODE_DELIVERY

class PDPEnv:
    """
    PDP ç¯å¢ƒï¼šè½¦è¾†å¿…é¡»å…ˆåˆ°å–è´§ç‚¹å–è´§åæ‰èƒ½é€è´§ï¼Œå—è½¦è¾†å®¹é‡çº¦æŸï¼Œç¯å¢ƒä¸­æ›´æ–°ç´¯è®¡è¡Œé©¶è·ç¦»æ—¶é‡‡ç”¨é¢„è®¡ç®—çš„åœ°å›¾çœŸå®è·ç¦»ã€‚
    """
    def __init__(self, vehicle_capacity=20):
        self.vehicle_capacity = vehicle_capacity

    def reset(self, data):
        """
        æ ¹æ®ç”Ÿæˆçš„æ•°æ®é‡ç½®ç¯å¢ƒï¼Œåˆå§‹åŒ–å„ç±»çŠ¶æ€
        è¿”å› state å­—å…¸ï¼ŒåŒ…å«ï¼š
          - locsã€node_typeã€order_mapã€order_load
          - current_node (è½¦è¾†å½“å‰ä½ç½®)ã€visited (å·²è®¿é—®èŠ‚ç‚¹)
          - pickup_doneã€delivery_doneï¼ˆè®¢å•çŠ¶æ€ï¼‰
          - current_loadï¼ˆè½¦è¾†è½½è·ï¼‰ã€total_distanceï¼ˆç´¯è®¡è¡Œé©¶è·ç¦»ï¼‰
          - doneï¼ˆæ˜¯å¦å®Œæˆæ‰€æœ‰è®¢å•ï¼‰
          - action_maskï¼ˆåˆæ³•åŠ¨ä½œ maskï¼‰
        """
        state = {}
        state["locs"] = data["locs"]
        state["node_type"] = data["node_type"]
        state["order_map"] = data["order_map"]
        state["order_load"] = data["order_load"]
        batch_size, num_nodes, _ = state["locs"].shape
        num_orders = data["num_orders"]

        state["current_node"] = torch.zeros(batch_size, dtype=torch.int64)  # åˆå§‹åœ¨ depot
        state["visited"] = torch.zeros(batch_size, num_nodes, dtype=torch.bool)
        state["visited"][:, 0] = True  # depot è§†ä¸ºå·²è®¿é—®

        state["pickup_done"] = torch.zeros(batch_size, num_orders, dtype=torch.bool)
        state["delivery_done"] = torch.zeros(batch_size, num_orders, dtype=torch.bool)
        state["current_load"] = torch.zeros(batch_size, dtype=torch.int32)
        state["total_distance"] = torch.zeros(batch_size, dtype=torch.float32)
        state["done"] = torch.zeros(batch_size, dtype=torch.bool)

        state["action_mask"] = self.update_action_mask(state)
        return state

    def update_action_mask(self, state):
        """
        æ ¹æ®å½“å‰çŠ¶æ€æ›´æ–°åŠ¨ä½œ maskï¼š
          - å·²è®¿é—®èŠ‚ç‚¹å’Œ depot å‡ä¸å¯é€‰
          - å¯¹äºå–è´§èŠ‚ç‚¹ï¼šå½“è½¦è¾†åŠ ä¸Šè¯¥è®¢å•è´Ÿè½½ä¸è¶…å‡º capacity æ—¶å…è®¸
          - å¯¹äºé€è´§èŠ‚ç‚¹ï¼šä»…å½“å¯¹åº”è®¢å•å·²å–è´§ä¸”æœªé€è´§æ—¶å…è®¸
        è¿”å›ï¼šBoolean tensor (batch_size, num_nodes)
        """
        batch_size, num_nodes, _ = state["locs"].shape
        mask = torch.zeros(batch_size, num_nodes, dtype=torch.bool)
        for b in range(batch_size):
            for n in range(num_nodes):
                if state["visited"][b, n]:
                    mask[b, n] = False
                    continue
                if state["node_type"][b, n] == NODE_DEPOT:
                    mask[b, n] = False
                    continue
                if state["node_type"][b, n] == NODE_PICKUP:
                    order_id = state["order_map"][b, n].item()
                    load = state["order_load"][b, order_id].item()
                    if state["current_load"][b] + load <= self.vehicle_capacity:
                        mask[b, n] = True
                    else:
                        mask[b, n] = False
                    continue
                if state["node_type"][b, n] == NODE_DELIVERY:
                    order_id = state["order_map"][b, n].item()
                    if state["pickup_done"][b, order_id] and (not state["delivery_done"][b, order_id]):
                        mask[b, n] = True
                    else:
                        mask[b, n] = False
                    continue
        return mask

    def step(self, state, actions, distance_matrix):
        """
        æ ¹æ®åŠ¨ä½œ actions æ›´æ–°çŠ¶æ€ï¼Œä½¿ç”¨é¢„å…ˆè®¡ç®—çš„ distance_matrix æ›´æ–°ç´¯è®¡è·ç¦»
        å‚æ•°:
          state: å½“å‰çŠ¶æ€å­—å…¸
          actions: tensor (batch_size,) å„å®ä¾‹é€‰å–çš„èŠ‚ç‚¹ç´¢å¼•
          distance_matrix: é¢„è®¡ç®—çš„è·ç¦»çŸ©é˜µ (num_nodes, num_nodes)
        è¿”å›æ›´æ–°åçš„ state
        """
        batch_size, num_nodes, _ = state["locs"].shape
        new_state = state.copy()
        new_state["visited"] = state["visited"].clone()
        new_state["pickup_done"] = state["pickup_done"].clone()
        new_state["delivery_done"] = state["delivery_done"].clone()
        new_state["current_load"] = state["current_load"].clone()
        new_state["total_distance"] = state["total_distance"].clone()
        new_state["done"] = state["done"].clone()

        for b in range(batch_size):
            if state["done"][b]:
                continue
            current = state["current_node"][b].item()
            next_node = actions[b].item()
            # ä½¿ç”¨åœ°å›¾çœŸå®è·ç¦»
            dist = distance_matrix[current][next_node]
            new_state["total_distance"][b] += dist
            new_state["current_node"][b] = actions[b]
            new_state["visited"][b, next_node] = True

            node_type = state["node_type"][b, next_node].item()
            if node_type == NODE_PICKUP:
                order_id = state["order_map"][b, next_node].item()
                new_state["pickup_done"][b, order_id] = True
                load = state["order_load"][b, order_id].item()
                new_state["current_load"][b] += load
            elif node_type == NODE_DELIVERY:
                order_id = state["order_map"][b, next_node].item()
                if state["pickup_done"][b, order_id]:
                    new_state["delivery_done"][b, order_id] = True
                    load = state["order_load"][b, order_id].item()
                    new_state["current_load"][b] -= load
            if torch.all(new_state["pickup_done"][b]) and torch.all(new_state["delivery_done"][b]):
                new_state["done"][b] = True

        new_state["action_mask"] = self.update_action_mask(new_state)
        return new_state

    def get_reward(self, state):
        """
        ä»¥è´Ÿç´¯è®¡è·ç¦»ä½œä¸ºå¥–åŠ±ï¼ˆç›®æ ‡ä¸ºæœ€å°åŒ–è¿è¾“è·ç¦»ï¼‰
        """
        return -state["total_distance"]

    def render(self, state):
        """
        æ‰“å°å½“å‰çŠ¶æ€ä¿¡æ¯ï¼Œä¾¿äºè§‚å¯Ÿ
        """
        batch_size, num_nodes, _ = state["locs"].shape
        for b in range(batch_size):
            print(f"å®ä¾‹ {b}:")
            print(f"  å½“å‰èŠ‚ç‚¹: {state['current_node'][b].item()}")
            print(f"  å½“å‰è½½è·: {state['current_load'][b].item()}")
            print(f"  ç´¯è®¡è·ç¦»: {state['total_distance'][b].item():.2f}")
            print(f"  pickup_done: {state['pickup_done'][b].tolist()}")
            print(f"  delivery_done: {state['delivery_done'][b].tolist()}")
            print(f"  done: {state['done'][b].item()}")
            print("------------")

def greedy_policy(state, env, distance_matrix):
    """
    è´ªå¿ƒç­–ç•¥ï¼šæ ¹æ®è·ç¦»çŸ©é˜µé€‰æ‹©å½“å‰åˆæ³•åŠ¨ä½œä¸­è·ç¦»æœ€è¿‘çš„èŠ‚ç‚¹
    å‚æ•°:
      state: å½“å‰çŠ¶æ€
      env: PDPEnvï¼ˆç”¨äºè·å–çŠ¶æ€ä¿¡æ¯ï¼‰
      distance_matrix: é¢„è®¡ç®—çš„è·ç¦»çŸ©é˜µ
    è¿”å›:
      tensor (batch_size,) å„å®ä¾‹é€‰å–çš„èŠ‚ç‚¹ç´¢å¼•
    """
    batch_size, num_nodes, _ = state["locs"].shape
    actions = torch.zeros(batch_size, dtype=torch.int64)
    for b in range(batch_size):
        if state["done"][b]:
            actions[b] = state["current_node"][b]
            continue
        current = state["current_node"][b].item()
        # ä»å½“å‰èŠ‚ç‚¹åœ¨ distance_matrix ä¸­å–å‡ºè·ç¦»å‘é‡
        dists = torch.tensor(distance_matrix[current])
        mask = state["action_mask"][b]
        dists[~mask] = float("inf")
        if torch.all(torch.isinf(dists)):
            actions[b] = current
        else:
            actions[b] = torch.argmin(dists)
    return actions

def aco_policy(state, env, distance_matrix, num_ants=20, num_iters=30, alpha=1.0, beta=5.0, rho=0.1, q=100):
    batch_size, num_nodes, _ = state["locs"].shape
    actions = torch.zeros(batch_size, dtype=torch.int64)

    for b in range(batch_size):
        if state["done"][b]:
            actions[b] = state["current_node"][b]
            continue

        pheromone = torch.ones((num_nodes, num_nodes)) * 0.1
        best_path = None
        best_length = float('inf')

        for _ in range(num_iters):
            for _ in range(num_ants):
                temp_state = {k: v.clone() if isinstance(v, torch.Tensor) else v for k,v in state.items()}
                path = []
                total_length = 0.0
                current_node = temp_state["current_node"][b].item()

                while not temp_state["done"][b]:
                    mask = temp_state["action_mask"][b]
                    probs = torch.zeros(num_nodes)
                    for j in range(num_nodes):
                        if mask[j]:
                            probs[j] = (pheromone[current_node][j]**alpha) * ((1.0/distance_matrix[current_node][j])**beta)
                    if probs.sum() == 0:
                        break
                    probs /= probs.sum()
                    next_node = torch.multinomial(probs, 1).item()
                    path.append(next_node)
                    total_length += distance_matrix[current_node][next_node]
                    temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)
                    current_node = next_node

                if total_length < best_length:
                    best_length = total_length
                    best_path = path

            pheromone *= (1 - rho)
            if best_path:
                for i in range(len(best_path)-1):
                    u, v = best_path[i], best_path[i+1]
                    pheromone[u][v] += q / (best_length + 1e-5)

        if best_path and len(best_path) > 0:
            actions[b] = best_path[0]
        else:
            actions[b] = state["current_node"][b]

    return actions

def greedy_then_aco_policy(state, env, distance_matrix, num_ants=20, num_iters=30, alpha=1.0, beta=5.0, rho=0.1, q=100):
    """
    è´ªå¿ƒ-èšç¾¤æ··åˆç­–ç•¥ï¼š
    å…ˆç”¨è´ªå¿ƒç”Ÿæˆåˆè§£ï¼Œå¹¶ç”¨åˆè§£åˆå§‹åŒ–ä¿¡æ¯ç´ ï¼Œ
    å†ç”¨èšç¾¤ç®—æ³•è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚
    """
    batch_size, num_nodes, _ = state["locs"].shape
    actions = torch.zeros(batch_size, dtype=torch.int64)

    for b in range(batch_size):
        if state["done"][b]:
            actions[b] = state["current_node"][b]
            continue

        # Step1: è´ªå¿ƒåˆè§£
        temp_state = {k: v.clone() if isinstance(v, torch.Tensor) else v for k,v in state.items()}
        greedy_path = []
        current_node = temp_state["current_node"][b].item()

        while not temp_state["done"][b]:
            mask = temp_state["action_mask"][b]
            dists = torch.tensor(distance_matrix[current_node])
            dists[~mask] = float('inf')
            if torch.all(torch.isinf(dists)):
                break
            next_node = torch.argmin(dists).item()
            greedy_path.append(next_node)
            temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)
            current_node = next_node

        # Step2: åˆå§‹åŒ–ä¿¡æ¯ç´ 
        pheromone = torch.ones((num_nodes, num_nodes)) * 0.1
        for i in range(len(greedy_path)-1):
            u, v = greedy_path[i], greedy_path[i+1]
            pheromone[u][v] += q / (len(greedy_path)+1e-5)

        best_path = None
        best_length = float('inf')

        # Step3: èšç¾¤å¤šè½®æœç´¢
        for _ in range(num_iters):
            for _ in range(num_ants):
                temp_state = {k: v.clone() if isinstance(v, torch.Tensor) else v for k,v in state.items()}
                path = []
                total_length = 0.0
                current_node = temp_state["current_node"][b].item()

                while not temp_state["done"][b]:
                    mask = temp_state["action_mask"][b]
                    probs = torch.zeros(num_nodes)
                    for j in range(num_nodes):
                        if mask[j]:
                            probs[j] = (pheromone[current_node][j]**alpha) * ((1.0/distance_matrix[current_node][j])**beta)
                    if probs.sum() == 0:
                        break
                    probs /= probs.sum()
                    next_node = torch.multinomial(probs, 1).item()
                    path.append(next_node)
                    total_length += distance_matrix[current_node][next_node]
                    temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)
                    current_node = next_node

                if total_length < best_length:
                    best_length = total_length
                    best_path = path

            pheromone *= (1 - rho)
            if best_path:
                for i in range(len(best_path)-1):
                    u, v = best_path[i], best_path[i+1]
                    pheromone[u][v] += q / (best_length + 1e-5)

        if best_path and len(best_path) > 0:
            actions[b] = best_path[0]
        else:
            actions[b] = state["current_node"][b]

    return actions

def greedy_then_mmas_policy(state, env, distance_matrix, num_ants=20, num_iters=30, alpha=1.0, beta=5.0, rho=0.1, q=100, p_best=0.005):
    """
    è´ªå¿ƒ-Max-Minèšç¾¤æ··åˆç­–ç•¥ï¼š
    å…ˆç”¨è´ªå¿ƒç”Ÿæˆåˆè§£ï¼Œå¹¶ä»¥åˆè§£è·¯å¾„é•¿åº¦å½’ä¸€åŒ–è®¾ç½®ä¿¡æ¯ç´ ä¸Šé™ï¼Œ
    ç„¶ååŸºäºMax-Min Ant Systemè¿›è¡Œè·¯å¾„ä¼˜åŒ–ã€‚
    """
    batch_size, num_nodes, _ = state["locs"].shape
    actions = torch.zeros(batch_size, dtype=torch.int64)

    for b in range(batch_size):
        if state["done"][b]:
            actions[b] = state["current_node"][b]
            continue

        # Step1: è´ªå¿ƒåˆè§£
        temp_state = {k: v.clone() if isinstance(v, torch.Tensor) else v for k, v in state.items()}
        greedy_path = []
        current_node = temp_state["current_node"][b].item()

        while not temp_state["done"][b]:
            mask = temp_state["action_mask"][b]
            dists = torch.tensor(distance_matrix[current_node])
            dists[~mask] = float('inf')
            if torch.all(torch.isinf(dists)):
                break
            next_node = torch.argmin(dists).item()
            greedy_path.append(next_node)
            temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)
            current_node = next_node

        # è®¡ç®—è´ªå¿ƒè·¯å¾„æ€»é•¿åº¦
        greedy_length = sum(distance_matrix[greedy_path[i]][greedy_path[i+1]] for i in range(len(greedy_path)-1))

        # Step2: åˆå§‹åŒ–ä¿¡æ¯ç´ 
        tau_max = q / (greedy_length + 1e-5)
        pheromone = torch.ones((num_nodes, num_nodes)) * tau_max

        avg = num_nodes / 2
        tau_min = tau_max * (1 - p_best) / (avg - 1)

        best_path = None
        best_length = float('inf')

        # Step3: Max-Minèšç¾¤å¤šè½®æœç´¢
        for _ in range(num_iters):
            paths = []
            lengths = []

            for _ in range(num_ants):
                temp_state = {k: v.clone() if isinstance(v, torch.Tensor) else v for k, v in state.items()}
                path = []
                total_length = 0.0
                current_node = temp_state["current_node"][b].item()

                while not temp_state["done"][b]:
                    mask = temp_state["action_mask"][b]
                    probs = torch.zeros(num_nodes)
                    for j in range(num_nodes):
                        if mask[j]:
                            probs[j] = (pheromone[current_node][j]**alpha) * ((1.0/distance_matrix[current_node][j])**beta)
                    if probs.sum() == 0:
                        break
                    probs /= probs.sum()
                    next_node = torch.multinomial(probs, 1).item()
                    path.append(next_node)
                    total_length += distance_matrix[current_node][next_node]
                    temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)
                    current_node = next_node

                paths.append(path)
                lengths.append(total_length)

            # é€‰æœ¬è½®æœ€ä¼˜
            min_idx = torch.argmin(torch.tensor(lengths)).item()
            if lengths[min_idx] < best_length:
                best_length = lengths[min_idx]
                best_path = paths[min_idx]

            # ä¿¡æ¯ç´ æŒ¥å‘
            pheromone *= (1 - rho)

            # åªå¼ºåŒ–æœ€ä¼˜è·¯å¾„
            if best_path and len(best_path) > 1:
                for i in range(len(best_path)-1):
                    u, v = best_path[i], best_path[i+1]
                    pheromone[u][v] += q / (best_length + 1e-5)

            # Max-Minè¾¹ç•Œæ§åˆ¶
            pheromone = torch.clamp(pheromone, min=tau_min, max=tau_max)

        # é€‰æ‹©åŠ¨ä½œ
        if best_path and len(best_path) > 0:
            actions[b] = best_path[0]
        else:
            actions[b] = state["current_node"][b]

    return actions

def high_quality_greedy_initial_path(state, env, distance_matrix, b):
    temp_state = {k: v.clone() for k, v in state.items()}
    path = []

    while not temp_state["done"][b]:
        mask = temp_state["action_mask"][b]
        current_node = temp_state["current_node"][b].item()
        best_next_node, best_cost = None, float('inf')

        for next_node in mask.nonzero(as_tuple=True)[0].tolist():
            cost = distance_matrix[current_node][next_node]
            if cost < best_cost:
                best_cost = cost
                best_next_node = next_node

        if best_next_node is None:
            break

        path.append(best_next_node)
        temp_state = env.step(temp_state, torch.tensor([best_next_node]), distance_matrix)

    return path, temp_state["total_distance"][b].item()


def stable_greedy_then_aco_policy(state, env, distance_matrix,
                                       num_ants=30, num_iters=60, alpha=1.0, beta=4.0, rho=0.15, q=100):
    batch_size, num_nodes, _ = state["locs"].shape
    actions = torch.zeros(batch_size, dtype=torch.int64)

    for b in range(batch_size):
        if state["done"][b]:
            actions[b] = state["current_node"][b]
            continue

        # Step1: é«˜è´¨é‡è´ªå¿ƒåˆå§‹è·¯å¾„
        greedy_path, greedy_length = high_quality_greedy_initial_path(state, env, distance_matrix, b)

        # Step2: åˆå§‹ä¿¡æ¯ç´ 
        pheromone = torch.ones((num_nodes, num_nodes)) * 0.1
        for i in range(len(greedy_path) - 1):
            pheromone[greedy_path[i]][greedy_path[i + 1]] += q / (greedy_length + 1e-5)

        best_path, best_length = greedy_path, greedy_length

        # Step3: èšç¾¤æœç´¢
        for _ in range(num_iters):
            paths, lengths = [], []

            for _ in range(num_ants):
                temp_state = {k: v.clone() for k, v in state.items()}
                path, total_length = [], 0.0

                while not temp_state["done"][b]:
                    current_node = temp_state["current_node"][b].item()
                    mask = temp_state["action_mask"][b]
                    feasible_nodes = mask.nonzero(as_tuple=True)[0].tolist()
                    probs = torch.tensor([(pheromone[current_node][j] ** alpha) * ((1.0 / distance_matrix[current_node][j]) ** beta) for j in feasible_nodes])
                    probs /= probs.sum()
                    next_node = feasible_nodes[torch.multinomial(probs, 1).item()]
                    path.append(next_node)
                    total_length += distance_matrix[current_node][next_node]
                    temp_state = env.step(temp_state, torch.tensor([next_node]), distance_matrix)

                paths.append(path)
                lengths.append(total_length)

            min_idx = torch.argmin(torch.tensor(lengths)).item()
            if lengths[min_idx] < best_length:
                best_length, best_path = lengths[min_idx], paths[min_idx]

            pheromone *= (1 - rho)
            for update_path in [paths[min_idx], best_path]:
                for i in range(len(update_path) - 1):
                    u, v = update_path[i], update_path[i + 1]
                    pheromone[u][v] += q / (best_length + 1e-5)

        next_node = best_path[0] if best_path else state["current_node"][b]
        actions[b] = next_node

    return actions


def rollout(env, state, distance_matrix, policy=greedy_policy, max_steps=50):
    """
    æ ¹æ® policy è¿›è¡Œ rolloutï¼Œç›´è‡³æ‰€æœ‰å®ä¾‹å®Œæˆæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°ã€‚
    æ‰“å°æ¯ä¸€æ­¥çš„è·¯å¾„ä¿¡æ¯ã€é…é€çŠ¶æ€ã€‚
    è¿”å›æœ€ç»ˆçŠ¶æ€å’Œè½¨è¿¹è®°å½•ï¼ˆæ¯å®ä¾‹æ¯æ­¥é€‰æ‹©çš„èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨ï¼‰ã€‚
    """
    batch_size = state["locs"].shape[0]
    trajectories = [[] for _ in range(batch_size)]
    steps = 0

    while not torch.all(state["done"]) and steps < max_steps:
        actions = policy(state, env, distance_matrix)
        for b in range(batch_size):
            if state["done"][b]:
                continue
            current = state["current_node"][b].item()
            next_node = actions[b].item()
            dist = distance_matrix[current][next_node]
            # print(f"\nğŸ§­ å®ä¾‹ {b} - æ­¥éª¤ {steps + 1}")
            # print(f"  ä»èŠ‚ç‚¹ {current} -> {next_node}ï¼Œæœ¬æ¬¡è·ç¦»: {dist:.2f} km")
            # print(f"  å½“å‰ç´¯è®¡è·ç¦»: {state['total_distance'][b].item():.2f} km")
            # print(f"  å½“å‰è½½è·: {state['current_load'][b].item()}")
            # print(f"  å·²å–è´§è®¢å•: {state['pickup_done'][b].tolist()}")
            # print(f"  å·²é€è¾¾è®¢å•: {state['delivery_done'][b].tolist()}")

        for b in range(batch_size):
            trajectories[b].append(actions[b].item())

        state = env.step(state, actions, distance_matrix)
        steps += 1

    return state, trajectories